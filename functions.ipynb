{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfaa079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sawal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sawal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "# text mining\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer  # Snowball stemmer was chosen in favor of Porter Stemmer which is a bit more aggressive and tends to remove too much from a word\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS_FR = stopwords.words(\"french\")\n",
    "\n",
    "# data viz\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3e0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMMER_FR = SnowballStemmer(language='french')\n",
    "STEMMER_EN = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b28331",
   "metadata": {},
   "source": [
    "# Exercice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52f0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_for(file):\n",
    "    \n",
    "    \"\"\"This function parses a text file by using loop on each lines of the original file.\n",
    "    \n",
    "     Arguments:\n",
    "      file {str}  : path of txt file.\n",
    "     \n",
    "    Returns:\n",
    "      pd.DataFrame - Output for exercie 1\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # store all the id as keys and theirs values\n",
    "    keys = []\n",
    "    vals = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"id\"):\n",
    "            keys.append(line)\n",
    "        else:\n",
    "            vals.append(line)\n",
    "            \n",
    "    # match the keys and values with dict\n",
    "    d = {}\n",
    "    for i in lines:\n",
    "        tmp = []\n",
    "        if i in keys:\n",
    "            tmp.append(lines[lines.index(i)+1])\n",
    "            n= 2\n",
    "            if lines.index(i) < len(lines)-2:\n",
    "                while lines[lines.index(i)+n] not in keys:\n",
    "                    tmp.append(lines[lines.index(i)+n-1])\n",
    "                    n=n+1\n",
    "            d[i] = tmp\n",
    "    \n",
    "    # transpose the dicts in a datafram\n",
    "    df = pd.DataFrame(\n",
    "                    pd.DataFrame(dict([(key, pd.Series(val)) for key,val in d.items()])).transpose().stack()\n",
    "                ).reset_index()\n",
    "\n",
    "    df.columns = [\"id\", \"level_1\", \"value\"] \n",
    "    # proccess dataframe\n",
    "    df[[\"vars_values\", \"values\"]] = df.value.str.split(\",\", expand=True)\n",
    "    df[[\"vars_id\", \"id\"]] = df.id.str.split(\",\", expand=True)\n",
    "    df.drop([\"vars_id\", \"value\"], axis=1, inplace=True)\n",
    "    \n",
    "    # pivot the data\n",
    "    df[\"index\"] = df.index\n",
    "    df = df.pivot(index=[\"index\", \"id\"], columns=\"vars_values\", values=\"values\")\n",
    "    df = df.reset_index().rename_axis(None, axis=1).drop(\"index\", axis=1)\n",
    "    \n",
    "    # customize output table\n",
    "    cols = [col for col in df.columns]\n",
    "    last_col= cols[len(cols)-1]\n",
    "    for i in range(1, int(last_col.replace(\"v\",\"\")) +1):\n",
    "        if f\"v{i}\" not in cols:\n",
    "            df[f\"v{i}\"] = np.nan\n",
    "    cols = [col for col in df.columns]\n",
    "    # sort v1 v2 v3 v4 \n",
    "    cols.sort()        \n",
    "    return df.reindex(columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dabb1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_noloop_lines(file, test=False):\n",
    "    \n",
    "    \"\"\"This function parses a text file WITHOUT using loop on each lines of the original file.We use vectorization and arrays.\n",
    "    \n",
    "     Arguments:\n",
    "      file {str}  : path of txt file.\n",
    "     \n",
    "    Returns:\n",
    "      pd.DataFrame - Output for exercie 1\n",
    "\n",
    "    \"\"\"\n",
    "    if test:\n",
    "        df = pd.read_csv(file, header=None, sep='\\n,', engine='python')\n",
    "    else:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "\n",
    "    df.columns= [\"key\", \"value\"]\n",
    "    # find list of index id\n",
    "    list_index = df[df.key == \"id\"].index.to_list()\n",
    "    list_index_shift = list_index[1:] + [df.shape[0]]\n",
    "    # find how many times is repeated an id\n",
    "    list_repetition_value = np.array(list_index_shift) - np.array(list_index) - 1\n",
    "    # create arrays of list  id\n",
    "    array_list_id = df[df.key == \"id\"].value.apply(lambda x: [x]).values\n",
    "    # create list of id for the output table\n",
    "    list_id = (list_repetition_value*array_list_id).sum()\n",
    "    #pivot initial table\n",
    "    df_final = df.pivot(columns=\"key\", values=\"value\")\n",
    "    df_final[\"id\"] = df_final.id.shift()\n",
    "    # drop rows with nan\n",
    "    df_final = df_final.dropna(how=\"all\")\n",
    "    df_final = df_final.rename_axis(None, axis=1).reset_index(drop=True)\n",
    "    df_final[\"id\"] = list_id\n",
    "\n",
    "    # customize output table : add features v\\d that are missing\n",
    "    cols = [col for col in df_final.columns]\n",
    "    for i in range(1, int(cols[len(cols)-1].replace(\"v\",\"\")) +1):\n",
    "        if f\"v{i}\" not in df_final.columns:\n",
    "            df_final[f\"v{i}\"] = np.nan\n",
    "    cols = [col for col in df_final.columns]\n",
    "    # sort v1 v2 v3 v4 \n",
    "    cols.sort()        \n",
    "    return df_final.reindex(columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270f2ac",
   "metadata": {},
   "source": [
    "# Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15e7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the last day of month\n",
    "def last_day_month(DATE):\n",
    "    \"\"\"This function returns the last date of month.\n",
    "    \n",
    "     Arguments:\n",
    "      DATE {Timestamp} \n",
    "     \n",
    "    Returns:\n",
    "      Timestamp - last date of month. \n",
    "      --------------------------------\n",
    "      Example : 2021-11-30\n",
    "      \n",
    "      \"\"\"\n",
    "      \n",
    "    days = calendar.monthrange(DATE.year, DATE.month)[1]\n",
    "    return DATE.replace(day=days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e953ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(df):\n",
    "    \n",
    "    \"\"\"This function return the sequences of start date and end date by id\n",
    "        \n",
    "    Arguments:\n",
    "      df {pd.DataFrame} -- raw dataframe - parsed dates columns\n",
    "     \n",
    "    Returns:\n",
    "      pd.DataFrame\n",
    "      \n",
    "        Exemple\n",
    "        |id  |mois        |end         |\n",
    "        |----|------------|------------|\n",
    "        |1   |2017-11-01  |2017-11-30  |\n",
    "        |1   |2017-12-01  |2017-12-31  |\n",
    "        |1   |2018-01-01  |2018-01-31  |\n",
    "        |1   |2018-02-01  |2018-02-28  |\n",
    "        |2   |2017-11-01  |2017-11-30  |\n",
    "        |2   |2017-12-01  |2017-12-31  |\n",
    "        |2   |2018-01-01  |2018-01-31  |\n",
    "        |2   |2018-02-01  |2018-02-28  |\n",
    "      \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get min of date and max of date \n",
    "    ref = df.agg({'date_debut':'min', 'date_fin':'max'})\n",
    "    # define start of months\n",
    "    low = ref[0].replace(day=1)\n",
    "    up = ref[1].replace(day=1)\n",
    "    # create a liste of months from min to max\n",
    "    liste = []\n",
    "    d={}\n",
    "    while low!=up:\n",
    "        liste.append(low)\n",
    "        low = low + relativedelta(months=+1)\n",
    "    liste.append(up)\n",
    "    liste\n",
    "    # attach id id to the list of months\n",
    "    for id in df.id:\n",
    "        d[id] = liste\n",
    "    # transform the dict to a dataframe\n",
    "    data = pd.DataFrame(\n",
    "                    pd.DataFrame(dict([(key, pd.Series(val)) for key,val in d.items()])).transpose().stack()\n",
    "                ).reset_index()\n",
    "\n",
    "    data = data.drop(\"level_1\", axis=1)\\\n",
    "               .rename(columns={'level_0':'id', 0:'mois'})\n",
    "    \n",
    "    data[\"end\"] = data[\"mois\"].apply(last_day_month)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d862dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_matching(df_baseline, df):\n",
    "    \n",
    "      \n",
    "    \"\"\"This function return the NB_arret by id and by month\n",
    "        \n",
    "    Arguments:\n",
    "      df_baseline {pd.DataFrame} -- sequences of dates by id \n",
    "      df {pd.DataFrame} -- raw dataframe - parsed dates columns\n",
    "     \n",
    "    Returns:\n",
    "      pd.DataFrame\n",
    "      \n",
    "    \"\"\"\n",
    "    # merge baseline and original table\n",
    "    df_merge = pd.merge(df_baseline, df, on=\"id\", how='left')     \n",
    "    # limit date_fin by month \n",
    "    cond2 = (df_merge['mois'] < df_merge['date_debut']) & (df_merge['date_fin'] > df_merge['end'])\n",
    "    df_merge.loc[cond2, 'date_fin'] =   df_merge['end']\n",
    "    #df_merge.drop_duplicates(subset=None, keep=\"first\", inplace=True)\n",
    "\n",
    "    # Therefore, specify the beginning after limitation\n",
    "    cond3 =  df_merge['date_fin'].dt.month - df_merge['date_debut'].dt.month !=0\n",
    "    df_merge.loc[cond3, 'date_debut'] =   df_merge['mois']\n",
    "    #df_merge.drop_duplicates(subset=None, keep=\"first\", inplace=True)\n",
    "\n",
    "    # Assign NA to matching datef_fin ad date_debut\n",
    "    cond4 = (df_merge['date_fin'] < df_merge['date_debut']) |  (df_merge['date_fin'] < df_merge['mois'])\n",
    "    df_merge.loc[cond4, [\"date_debut\", \"date_fin\"]] = None\n",
    "\n",
    "    # drop non overlapping months \n",
    "    cond5 = df_merge[(df_merge['date_debut'] == df_merge['mois']) &  (df_merge['date_fin'] == df_merge['end'])].index\n",
    "    df_merge.drop(cond5 , inplace=True)\n",
    "\n",
    "    # keeep only last end of month\n",
    "    df_merge = df_merge.sort_values(by=[\"id\", \"mois\", \"end\", \"date_debut\", \"date_fin\"], ascending=True)\\\n",
    "                       .drop_duplicates(subset=[\"id\",\"mois\",\"end\"], keep=\"last\")\n",
    "\n",
    "    # compute NB_ arret\n",
    "    df_merge['NB_arret'] =  ((df_merge['date_fin'] - df_merge['date_debut']).dt.days +1).fillna(0).astype(int)\n",
    "\n",
    "    df_merge = df_merge.reset_index(drop=True)\\\n",
    "                       .drop(\"end\", axis=1)\\\n",
    "                       .rename({'date_debut':'date_debut_mois', 'date_fin':'date_fins_mois'})\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9190997",
   "metadata": {},
   "source": [
    "# Exercice 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78b2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_prep(row, \n",
    "                        word_blacklist,\n",
    "                        regex_replace, \n",
    "                        colonne:str =None) :\n",
    "    \"\"\"\n",
    "    This function treats the input string by going through the following steps:\n",
    "        - Language detection\n",
    "        - Remove punctuation and special characters\n",
    "        - Tekenization\n",
    "        - Stop-word removal\n",
    "        - Stemming\n",
    "        - ASCII folding.\n",
    "    \n",
    "    Args:\n",
    "      \n",
    "        row (str) : The input string to be treated.\n",
    "        word_blacklist (list[str]) : additional stop-word\n",
    "        regex_replace (Dict[str, str]) ::characters to remove\n",
    "         colonne :  name of the colonne to prepare.  Default = None,If None, the input is a string.\n",
    "      \n",
    "    Returns:\n",
    "      str : The treated version of the string. \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if colonne == None :\n",
    "      s = str(row)\n",
    "    else :\n",
    "      s=row[colonne]\n",
    "    \n",
    "    # in the default case use the English stop-words and stemmer\n",
    "    stemmer = STEMMER_FR\n",
    "    stop_words =  word_blacklist + STOPWORDS_FR\n",
    "\n",
    "    \n",
    "    # convert to lowercase, just to be sure :)\n",
    "    s = s.lower()\n",
    "    \n",
    "    # check if the language isn't French and switch to the English:\n",
    "    # No need because all data are in french\n",
    "    \n",
    "    \"\"\"s_lang = detect(s)\n",
    "    if s_lang[0]!=\"fr\":\n",
    "       stemmer = STEMMER_EN\n",
    "       stop_words = word_blacklist\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # remove punctuation\n",
    "    s_clean = s.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "\n",
    "    # tokenize the string into words\n",
    "    s_tokens = word_tokenize(s_clean)\n",
    "\n",
    "    # remove the stop-word tokens\n",
    "    s_tokens_no_stop = [word for word in s_tokens if word not in stop_words]\n",
    "    \n",
    "    # join the stemmed tokens together and ASCII fold\n",
    "    s_tokens_stemmed = [stemmer.stem(word) for word in s_tokens_no_stop]\n",
    "    s_ascii = unidecode(\" \".join(s_tokens_stemmed))\n",
    "    \n",
    "    for regex, replace in regex_replace.items():\n",
    "      s_ascii = re.sub(regex, replace, s_ascii)\n",
    "\n",
    "    return(s_ascii.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400911c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def experimenter(data, classifiers):\n",
    "        \n",
    "    \"\"\"A function to run multiple algorithms in succession with their default parameters \n",
    "    without hyperparameter tuning\n",
    "     Arguments:\n",
    "      data {pd.DataFrame} -- input dataframe\n",
    "      classifiers {List} --lists of classes - algorithms .Example :[LogisticRegression(),  DecisionTreeClassifier()]\n",
    "     \n",
    "    Returns:\n",
    "      print AUC in the test set and the training set.\n",
    "      \n",
    "    \"\"\"\n",
    "    for classifier in classifiers:\n",
    "        # split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.description_clean, data.target, test_size = 0.2, random_state=202209)\n",
    "        \n",
    "        # vectorization\n",
    "        vectorizer = CountVectorizer(ngram_range=(1,1), min_df=3, max_df=0.9, strip_accents='unicode', analyzer=\"word\")\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "        \n",
    "        # entrainement\n",
    "        model = classifier\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        #calcul de la prédiction sur l'échantillon test\n",
    "\n",
    "        pred_proba_test = model.predict_proba(X_test)[:,1]\n",
    "        pred_proba_train = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "        # calculer de metric\n",
    "        print(f\"----------classifier : {classifier} ----------\")\n",
    "        print(\"AUC TRAIN:\", roc_auc_score(y_train, pred_proba_train))\n",
    "        print(\"AUC TEST:\", roc_auc_score(y_test,pred_proba_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae36c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_confusion_matrix(y_test, y_predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function return a friendly confusion matrix - easy to read\n",
    "    \n",
    "    Args:\n",
    "      \n",
    "        y_test {pd.Series of int} : target /label \n",
    "        y_predicted {pd.Series of int} : predicted label\n",
    "    Returns:\n",
    "      matplotlib.pyplot.figure : a table. \n",
    "    \"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).astype(int).ravel()\n",
    "    print(\"Accuracy:\", round((tp + tn)/(tp+tn+fp+fn),2))\n",
    "    print(\"Recall:\", round(tp /(tp+fn),2))\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.clf()\n",
    "    plt.imshow(cm, interpolation='nearest',cmap=plt.cm.Wistia)\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Matrice de confusion')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=45)\n",
    "    plt.yticks(tick_marks, classNames)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    "\n",
    "    for i in range(2):\n",
    "      for j in range(2):\n",
    "          plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e68aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_individuelle(input):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function predicts the class given  a raw input description\n",
    "    \n",
    "    Args:\n",
    "      \n",
    "        input (str) : The input string \n",
    "    Returns:\n",
    "      Bolean : description class. \n",
    "      \n",
    "    \"\"\"\n",
    "    with open(\"./INTERMED/optimal_thr.sav\",\"rb\") as f:\n",
    "        optimal_thr =pickle.load(f)\n",
    "        \n",
    "    with open(\"./INTERMED/random_forest_model.sav\",\"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    with open(\"./INTERMED/vectorizer.sav\",\"rb\") as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    input = pd.DataFrame( [[input]],columns=[ 'Description'])\n",
    "\n",
    "    input = vectorizer.transform(input['Description'])\n",
    " \n",
    "    if  model.predict_proba(input)[:,1] <optimal_thr:\n",
    "        return False\n",
    "    else :\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d810776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_file(path=\"./INPUT/act_couv.csv\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function predicts the class given a overall csv file  \n",
    "    \n",
    "    Args:\n",
    "        path (str) : path of csv file\n",
    "    Returns:\n",
    "      file : csv\n",
    "    \"\"\"   \n",
    "    data = pd.read_csv(path, sep=\";\")\n",
    "    data[\"class_predite\"] = data.description.apply(prediction_individuelle)\n",
    "    data.to_csv(\"./OUTPUT/ct_couv_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdc12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
